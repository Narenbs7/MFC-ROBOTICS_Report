<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>EchoSwarm EEG Robotics</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <style>
    html { scroll-behavior: smooth; }
    .section { padding-top: 4rem; padding-bottom: 4rem; }
  </style>
</head>
<body class="bg-gray-50 text-gray-800 font-sans">

  <!-- Custom Banner -->
  <section class="project-banner">
    <div class="banner-container">
      <div class="banner-content">
        <div class="course-info">
          <p>22 AIE 214 - Introduction to AI Robotics<br>22 MAT 230 - Mathematics for Computing - 4</p>
        </div>
        
        <h1 class="project-title">
          "EchoSwarm" <span class="project-subtitle">EEG-Guided Swarm Robotics</span>
        </h1>
        
        <div class="project-description">
          <h1>Optimal Control and Distributed Coordination via Neural Signal Processing</h1>
        </div>
        
        <div class="review-badge">
          End Semester Final Review - April 2025
        </div>
      </div>
      
      <div class="team-card">
        <div class="team-header">
          <span class="team-title">Group - B6 (AIE)</span>
        </div>
        <ul class="team-members">
          <li><span class="member-name">Akhilesh M:</span> <span class="member-id">CB.SC.U4AIE23111</span></li>
          <li><span class="member-name">Naren B S:</span> <span class="member-id">CB.SC.U4AIE23152</span></li>
          <li><span class="member-name">Nitin Krishna:</span> <span class="member-id">CB.SC.U4AIE23156</span></li>
          <li><span class="member-name">Gaurav Mahesh:</span> <span class="member-id">CB.SC.U4AIE23176</span></li>
        </ul>
      </div>
    </div>
  </section>

<!-- Navigation Bar -->
<nav class="main-nav">
  <div class="nav-container">
    <div class="nav-logo">
      <span>Project Report</span>
    </div>
    <div class="nav-links">
      <a href="#abstract" class="nav-item">Abstract</a>
      <a href="#toc" class="nav-item">Contents</a>
      <a href="#intro" class="nav-item">Introduction</a>
      <a href="#literature" class="nav-item">Related Work</a>
      <a href="#methodology" class="nav-item">Methodology</a>
      <a href="#drone-simulation" class="nav-item">Simulation</a>
      <a href="#results" class="nav-item">Results</a>
      <a href="#demo" class="nav-item">Demo</a>
      <a href="#conclusion" class="nav-item">Conclusion</a>
      <a href="#references" class="nav-item">References</a>
    </div>
  </div>
</nav>

  <main class="container mx-auto px-4">
    <section id="abstract" class="section">
      <h2 class="text-3xl font-bold text-gray-800">Abstract</h2>
      <p class="mt-4 text-lg text-justify leading-relaxed px-2">
        The <strong>EchoSwarm</strong> project introduces a novel interface between human neural signals and autonomous multi-robot systems by integrating <strong>EEG-based emotion recognition</strong> with <strong>real-time drone coordination</strong>. This system enables users to influence swarm behavior using nothing but their brainwaves, specifically emotions such as <em>Happiness</em>, <em>Sadness</em>, <em>Fear</em>, and <em>Neutrality</em>.
      
        <br><br>
        Using the publicly available <strong>SEED-IV dataset</strong>, we processed multi-channel EEG signals recorded from 15 subjects across 72 emotion-evoking trials. Each EEG signal underwent advanced preprocessing techniques including <strong>Butterworth band-pass filtering</strong>, <strong>spectral analysis</strong> via <em>Welch’s method</em>, and <strong>fractal dimension extraction</strong> (Higuchi and Katz). These features were then fed into a hybrid classification framework involving <strong>ConvLSTM2D</strong>, <strong>Random Forests</strong>, and <strong>Support Vector Machines (SVM)</strong> to decode the user's emotional state with high accuracy.
      
        <br><br>
        Once classified, each emotional state is mapped to a predefined geometric formation in a swarm of drones simulated using <strong>CoppeliaSim</strong>. For instance, a detected “Happy” state prompts the drones to arrange into a <em>circle</em>, while a “Fear” state triggers a <em>star</em> pattern. The formation logic and movement coordination are achieved through <strong>Particle Swarm Optimization (PSO)</strong> and <strong>convex optimization algorithms</strong>, ensuring that the drones maintain stable, collision-free configurations in real time.
      
        <br><br>
        Communication between the emotion classifier and the simulation engine is handled via <strong>ZMQ (ZeroMQ)</strong> sockets, enabling a responsive, closed-loop interaction where real-time EEG input continuously informs drone behavior. This creates an immersive, intuitive, and natural method for engaging with robotic swarms—controlled entirely by human thought.
      
        <br><br>
        The uniqueness of EchoSwarm lies in its hands-free approach to robotic control, pushing the boundaries of <strong>brain-computer interface (BCI)</strong> applications. This framework demonstrates potential in various domains:
        <ul class="list-disc list-inside mt-2 ml-6">
          <li><strong>Therapeutic Robotics</strong>: Emotionally responsive companions for mental health and stress monitoring</li>
          <li><strong>Interactive Art</strong>: Visualizing human emotion through intelligent drone choreography</li>
          <li><strong>Emergency Signaling</strong>: Triggering autonomous SOS formations during distress detection</li>
          <li><strong>Assistive Technology</strong>: Supporting users with physical limitations by providing thought-controlled interfaces</li>
        </ul>
      
        <br>
        Beyond its immediate functionality, EchoSwarm exemplifies the future of multi-modal AI systems—those that sense, interpret, and act in harmony with human emotional states. As such, it not only highlights the synergy between <strong>AI, neuroscience, and robotics</strong>, but also lays the groundwork for future applications in smart cities, emotion-aware automation, and neuroadaptive environments.
      
        <br><br>
        Future enhancements include real-time EEG acquisition using wearable hardware, adaptive learning for personalized emotional calibration, and deployment on physical drones with on-board intelligence, bringing us closer to the next frontier in human-robot collaboration.
      </p>
      
    </section>

    <section id="toc" class="toc-section">
      <h2 class="toc-title">Table of Contents</h2>
      <table class="toc-table">
        <tbody>
          <tr><td><a href="#abstract">Abstract</a></td></tr>
          <tr><td><a href="#intro">Introduction</a></td></tr>
          <tr><td><a href="#literature">Related Work</a></td></tr>
          <tr><td><a href="#methodology">Methodology</a></td></tr>
          <tr><td><a href="#drone-simulation">Simulation</a></td></tr>
          <tr><td><a href="#results">Results</a></td></tr>
          <tr><td><a href="#demo">Demo</a></td></tr>
          <tr><td><a href="#conclusion">Conclusion</a></td></tr>
          <tr><td><a href="#references">References</a></td></tr>
        </tbody>
      </table>
    </section>

    <section id="intro" class="section">
      <h2 class="text-3xl font-bold">Introduction</h2>
      <p class="mt-4 text-lg text-justify leading-relaxed px-2">
        In the rapidly evolving field of robotics, particularly within multi-agent systems, there is an increasing need for natural and intuitive human-robot interaction interfaces. Traditional robotic control systems rely on physical hardware such as joysticks, buttons, or scripted input commands, which limit accessibility and responsiveness in high-stakes environments like search and rescue, defense, or assistive technology.
      
        <br><br>
        <strong>EchoSwarm</strong> redefines this interaction by integrating <strong>brain-computer interface (BCI)</strong> technology with autonomous drone swarms, allowing users to command robotic agents solely through their emotional states. These emotions—<em>Happy, Sad, Fear, and Neutral</em>—are decoded from EEG (Electroencephalography) signals, processed and classified using advanced deep learning and ensemble machine learning models. Each detected emotion is directly linked to a predefined geometric formation that the drone swarm assumes in real-time, creating a hands-free, emotion-driven control framework.
      
        <br><br>
        The key idea behind this project is to use human affect as a high-level control signal. Emotional states, which often reflect intent or urgency, are rich sources of information for context-aware systems. By classifying emotional states using EEG features and mapping them to drone behaviors, we provide a responsive and adaptive control system capable of interpreting human states without explicit commands.
      
        <br><br>
        To implement this, we leveraged the <strong>SEED-IV EEG dataset</strong>, which captures real responses to emotion-evoking stimuli. The EEG data is preprocessed to reduce noise, extract relevant features (e.g., <strong>power spectral density</strong>, <strong>fractal dimensions</strong>, <strong>entropy measures</strong>), and formatted for temporal modeling. We employ <strong>ConvLSTM2D</strong> for spatiotemporal learning, alongside <strong>Random Forest</strong> and <strong>SVM</strong> classifiers to benchmark and validate performance.
      
        <br><br>
        Once an emotion is predicted, we use optimization strategies—specifically <strong>Particle Swarm Optimization (PSO)</strong> and <strong>convex optimization</strong>—to generate and maintain drone formations that reflect the user's current emotional state. This formation data is transmitted to <strong>CoppeliaSim</strong>, a high-fidelity robotics simulator, where virtual drones form shapes such as <em>circle (happy)</em>, <em>triangle (sad)</em>, <em>star (fear)</em>, and <em>square (neutral)</em>.
      
        <br><br>
        The project’s novelty lies not only in using EEG signals for drone control but in mapping abstract human affect to structured robotic motion through a fully integrated, real-time simulation loop. <strong>ZeroMQ (ZMQ)</strong> facilitates efficient messaging between the EEG model and simulation backend, allowing dynamic responses to changing emotional states.
      
        <br><br>
        Through EchoSwarm, we aim to showcase the potential of emotion-driven control in multi-agent systems, offering practical pathways for future innovations in <em>therapy, education, stress-detection, adaptive UI systems</em>, and <em>interactive entertainment</em>. By merging neuroscience, artificial intelligence, and robotics, we move a step closer to the vision of emotionally intelligent machines that can collaborate with humans not just through commands, but through understanding.
      </p>
      
    </section>

    <section id="literature" class="section bg-gray-100 rounded-xl shadow-md p-6">
      <h2 class="text-3xl font-bold">Literature Review / Related Work</h2>
      <div class="mt-4 space-y-6 text-lg">
        <p class="mt-4 text-lg text-justify leading-relaxed px-2">
          The field of EEG-based emotion recognition and swarm robotics has grown significantly over the past decade, with key contributions shaping our understanding of how brain-computer interfaces (BCIs) and multi-agent control systems can be integrated. This section summarizes the foundational studies that inform our approach and highlights how <strong>EchoSwarm</strong> fills a unique interdisciplinary gap between them.
        
          <br><br>
          <strong>EEG-Based Emotion Recognition – Zheng et al. (2015):</strong>  
          Zheng and colleagues introduced the <strong>SEED dataset</strong>, which remains one of the most comprehensive and widely used benchmarks for emotion recognition from EEG data. Their work employed <em>Deep Belief Networks</em> (DBNs) to classify emotional states evoked by film stimuli. This dataset laid the groundwork for emotion-labeled EEG classification, providing the core input for our project. While effective, their focus remained purely on classification and did not extend into real-world applications or interactive control systems.
        
          <br><br>
          <strong>Multi-Robot Formation Control – Aydin et al. (2014):</strong>  
          This study focused on the design of coordination strategies for structured motion among multiple robots. Algorithms were developed for inter-agent communication, distance preservation, and decentralized control. However, emotional or cognitive input from humans was not considered as part of the control architecture. <strong>EchoSwarm</strong> builds upon this foundation by mapping human affective states to formation geometries, thus embedding human intent directly into swarm dynamics.
        
          <br><br>
          <strong>Convex Optimization in Robotics – Boyd & Vandenberghe (2004):</strong>  
          Their seminal work on convex optimization formalized how mathematical tools can be applied to solve control problems in robotics. Concepts such as trajectory planning, collision avoidance, and formation optimization are now widely used. In EchoSwarm, we extend their principles to a swarm of drones whose formations are driven by decoded human emotions, thereby applying these techniques in an emotionally intelligent control context.
        
          <br><br>
          <strong>Swarm Coordination via Optimization – Murray (2007):</strong>  
          This research proposed optimization-based frameworks for swarm control, addressing scalability and stability under distributed inputs. Murray emphasized the need for robustness in dynamic systems, a principle we adopt in EchoSwarm through our use of <strong>Particle Swarm Optimization (PSO)</strong> for live formation updates in response to emotion transitions.
        
          <br><br>
          <strong>Key Gap and Novelty of EchoSwarm:</strong>  
          While each of these studies contributes to either EEG-based recognition or robotic swarm control, none of them explore the direct translation of human affective signals into actionable robotic behaviors. <strong>EchoSwarm</strong> is one of the first systems to bridge this gap—fusing deep learning-based emotion recognition with real-time swarm formation control in a closed-loop simulation. This interdisciplinary integration opens new avenues for emotion-aware automation, therapeutic swarm robotics, and adaptive human-machine interaction.
        </p>
        
      </div>
    </section>
    

    <section id="methodology" class="section bg-white rounded-xl shadow-md p-6">
      <h2 class="text-3xl font-bold text-gray-800 mb-4">Methodology & Implementation</h2>
      
        <!-- Dataset Selection -->
        <h3 class="text-2xl font-semibold text-gray-700 mt-4 mb-2">1. Dataset Selection</h3>
        <p class="text-lg text-justify leading-relaxed px-2">
          We utilized the publicly available <strong>SEED-IV dataset</strong>, comprising EEG signals from 15 participants recorded over 3 sessions. Each session includes 24 trials eliciting one of four emotions—<em>Happy</em>, <em>Sad</em>, <em>Fear</em>, and <em>Neutral</em>. The signals were captured using a 62-channel ESI NeuroScan system at a sampling rate of 1000 Hz and later downsampled to 200 Hz for efficient processing.
        </p>
        <img src="images/seed1.png" alt="SEED-IV Dataset Overview" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
      
        <!-- EEG Signal Preprocessing -->
        <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">2. EEG Signal Preprocessing</h3>
        <p class="text-lg text-justify leading-relaxed px-2">
          Raw EEG signals were passed through a <strong>Butterworth Bandpass Filter (0.5–70 Hz)</strong> to eliminate noise and baseline drift. We then performed <strong>feature extraction</strong> using multiple methods:
        </p>
        <ul class="list-disc list-inside text-lg ml-8 my-2">
          <li><strong>Power Spectral Density (PSD)</strong> using Welch’s method</li>
          <li><strong>Differential Entropy (DE)</strong></li>
          <li><strong>Fractal Dimensions:</strong> Higuchi and Katz methods</li>
          <li><strong>Statistical Features:</strong> mean, variance, Hurst exponent</li>
        </ul>
        <img src="images/preprocess.PNG" alt="EEG Signal Preprocessing" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
      
        <!-- Emotion Classification -->
        <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">3. Emotion Classification</h3>
        <p class="text-lg text-justify leading-relaxed px-2">
          Extracted features were used to train three models:
        </p>
        <ul class="list-disc list-inside text-lg ml-8 my-2">
          <li><strong>ConvLSTM2D:</strong> Designed to capture spatiotemporal EEG dynamics using convolutional and LSTM layers.</li>
          <li><strong>Random Forest:</strong> Used as a baseline classifier to evaluate raw feature effectiveness.</li>
          <li><strong>SVM (RBF Kernel):</strong> Effective for non-linear classification of multi-dimensional EEG features.</li>
        </ul>
        <img src="images/convlstm.jfif" alt="ConvLSTM Model Architecture" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
      
        <!-- Optimization Algorithms -->
        <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">4. Swarm Control & Optimization</h3>
        <p class="text-lg text-justify leading-relaxed px-2">
          We implemented <strong>Particle Swarm Optimization (PSO)</strong> to determine optimal drone positions for shape formation based on the classified emotion. PSO minimizes formation error while maintaining collision-free motion through dynamic updates in each timestep. We also incorporated <strong>convex optimization constraints</strong> to ensure formation stability and avoid overlapping paths in tight spaces.
        </p>
      
        <!-- Real-Time Simulation -->
        <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">5. Real-Time Drone Simulation in CoppeliaSim</h3>
        <p class="text-lg text-justify leading-relaxed px-2">
          Drone behaviors were simulated using <strong>CoppeliaSim</strong>, leveraging its physics engine and remote API for realistic motion. Each emotion was mapped to a distinct drone shape:
        </p>
        <ul class="list-disc list-inside text-lg ml-8 my-2">
          <li><strong>😊 Happy → Circle</strong></li>
          <li><strong>😞 Sad → Triangle</strong></li>
          <li><strong>😨 Fear → Star</strong></li>
          <li><strong>😐 Neutral → Square</strong></li>
        </ul>
      
        <!-- Integration -->
        <p class="text-lg text-justify leading-relaxed px-2 mt-4">
          Real-time control is achieved using <strong>ZeroMQ (ZMQ)</strong>, which facilitates fast and scalable messaging between the emotion prediction module and the drone formation generator. The entire system operates in a loop, continuously reading EEG data, predicting emotion, and triggering corresponding drone behaviors.
        </p>
      </section>
      
    </section>

    <section id="drone-simulation" class="section bg-white rounded-xl shadow-md p-6">
      <h2 class="text-3xl font-bold text-gray-800 mb-4">Drone Simulation</h2>
      
      <!-- Overview -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-4 mb-2">1. Overview</h3>
      <p class="text-lg text-justify leading-relaxed px-2">
        This module of the project focuses on simulating and controlling a quadcopter (drone) in a 3D virtual environment using <strong>CoppeliaSim</strong> (formerly V-REP). The drone is controlled using a custom PID controller integrated with realistic particle-based propulsion dynamics, providing an educational yet practical approach to UAV flight control.
      </p>
      <img src="images/drone-simulation-overview.jpeg" alt="Drone Simulation Overview" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
    
      <!-- Components Added to the Drone -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">2. Components Added to the Drone</h3>
      
      <!-- PID-Based Control System -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.1. PID-Based Control System</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        We implemented a custom PID (Proportional-Integral-Derivative) controller to stabilize and navigate the drone. This includes:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li><strong>Vertical Control:</strong> Manages altitude by comparing current and target heights.</li>
        <li><strong>Horizontal Control:</strong> Adjusts the drone's pitch and roll to maintain lateral stability.</li>
        <li><strong>Rotational Control:</strong> Uses yaw correction to stabilize orientation.</li>
      </ul>
      <p class="text-lg text-justify leading-relaxed px-2 mt-2">
        <strong>Parameters Tuned:</strong>
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li><code>pParam</code>, <code>iParam</code>, <code>dParam</code>: PID gains.</li>
        <li><code>vParam</code>: Controls response to vertical speed (velocity damping).</li>
      </ul>
      
      <!-- Realistic Propulsion Using Particle Dynamics -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.2. Realistic Propulsion Using Particle Dynamics</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        To simulate real-world rotor thrust, we used particle emitters attached to each propeller:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li>Particle emission models air push by rotors.</li>
        <li>Emission rate is linked to motor velocity.</li>
        <li>Added randomness to simulate scattering using <code>particleScatteringAngle</code>.</li>
      </ul>
      <p class="text-lg text-justify leading-relaxed px-2 mt-2">
        <strong>Particle Physics Parameters:</strong>
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li>Particle size: 0.005</li>
        <li>Density: 8500</li>
        <li>Lifetime: 0.5 seconds</li>
        <li>Max particles: 50 per rotor</li>
        <li>Physics flags: collision, cyclic behavior, gravity ignore</li>
      </ul>
      
      <!-- Reactive Forces and Torques -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.3. Reactive Forces and Torques</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        Each emitted particle contributes to a net force and torque:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li>Net upward thrust calculated and applied based on emission rate and mass.</li>
        <li>Rotational torque generated depending on propeller direction (to simulate drag-based rotation counteraction).</li>
        <li>This approach simulates realistic physics, as opposed to directly applying forces.</li>
      </ul>
      
      <!-- Independent Target Object -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.4. Independent Target Object</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        A separate target object (<code>targetObj</code>) is used as a reference for the drone to follow. This object can be moved programmatically or interactively, allowing:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li>Dynamic flight paths</li>
        <li>Hovering at a specific location</li>
        <li>Testing response to trajectory changes</li>
      </ul>
      <p class="text-lg text-justify leading-relaxed px-2">
        Target is detached (<code>sim.setObjectParent(targetObj, -1, true)</code>) so it can move freely.
      </p>
      
      <!-- Fake Shadow Visualization -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.5. Fake Shadow Visualization</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        A disc-based visual shadow is added under the drone to enhance realism and aid in debugging and visualization:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li>Implemented using <code>sim.addDrawingObject</code></li>
        <li>Positioned at the drone's (x, y) coordinates with a fixed low z-coordinate.</li>
        <li>Updated every simulation step.</li>
      </ul>
      
      <!-- Propeller Motion -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.6. Propeller Motion</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        Each rotor has a simulated joint whose rotation is updated every time step:
      </p>
      <p class="text-lg px-4 py-2 bg-gray-100 rounded-md font-mono my-2 overflow-x-auto">
        sim.setJointPosition(propellerJoint, t*10)
      </p>
      <p class="text-lg text-justify leading-relaxed px-2">
        Helps simulate continuous rotation, aiding realism in visual feedback.
      </p>
      
      <!-- Custom Simulation Settings -->
      <h4 class="text-xl font-semibold text-gray-600 mt-4 mb-2">2.7. Custom Simulation Settings</h4>
      <p class="text-lg text-justify leading-relaxed px-2">
        The drone has configurable toggles for:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li><code>particlesAreVisible</code>: Toggles visibility of particle thrust.</li>
        <li><code>simulateParticles</code>: Enables/disables particle dynamics.</li>
        <li><code>fakeShadow</code>: Enables shadow rendering.</li>
      </ul>
      <p class="text-lg text-justify leading-relaxed px-2">
        These flags make the simulation adjustable for different testing scenarios.
      </p>
    </section>
    

    <section id="results" class="section bg-gray-100 rounded-xl shadow-md p-6">
      <h2 class="text-3xl font-bold">Results and Discussion</h2>
      
      <p class="mt-4 text-lg text-justify leading-relaxed px-2">
        The EchoSwarm system was evaluated on the SEED-IV EEG dataset using multiple machine learning models. The performance was assessed through both <strong>Leave-One-Subject-Out (LOSO) Cross-Validation</strong> and <strong>Stratified K-Fold Cross-Validation</strong> techniques to ensure robust generalization across subjects and trials.
        <br><br>
        The <strong>ConvLSTM2D model</strong> demonstrated superior performance with its ability to capture spatial and temporal dependencies in EEG signals. It achieved an average accuracy of <strong>~72%</strong> across all emotions. In comparison, <strong>Random Forest</strong> achieved around <strong>62%</strong>, and <strong>SVM</strong> achieved approximately <strong>68%</strong> accuracy using handcrafted features.
      </p>
    
      <!-- Accuracy Plot -->
      <img src="images/result.PNG" alt="Model Accuracy Comparison" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
    
      <!-- Confusion Matrix -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">Confusion Matrix</h3>
      <p class="text-lg text-justify leading-relaxed px-2">
        The confusion matrix below illustrates how well each emotion was classified by the ConvLSTM2D model. The diagonal values represent correctly predicted instances, while off-diagonal values indicate misclassifications. As seen, “Happy” and “Neutral” were identified with higher confidence, while “Sad” and “Fear” occasionally overlapped due to similar signal features.
      </p>
    
      <!-- Shape Formation Discussion -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">Drone Formation Mapping</h3>
      <p class="text-lg text-justify leading-relaxed px-2">
        Upon accurate emotion prediction, corresponding drone formations were generated in real-time inside CoppeliaSim. Each emotion is linked to a geometric configuration:
      </p>
      <ul class="list-disc list-inside text-lg ml-8 my-2">
        <li><strong>😊 Happy → Circle:</strong> Indicates positive engagement or excitement.</li>
        <li><strong>😞 Sad → Triangle:</strong> Symbolizes introspection or low energy states.</li>
        <li><strong>😨 Fear → Star:</strong> Denotes urgency or stress, simulating alert behavior.</li>
        <li><strong>😐 Neutral → Square:</strong> Represents balance or calmness.</li>
      </ul>
      <img src="images/emotion.PNG" alt="Emotion-based Drone Formations" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
    
      <!-- System Responsiveness -->
      <p class="text-lg text-justify leading-relaxed px-2 mt-4">
        The system achieved <strong>real-time responsiveness</strong> using ZeroMQ (ZMQ) to transmit emotion predictions and trigger corresponding drone movements. During each EEG trial, the swarm adapted its shape in under 0.5 seconds, validating the closed-loop control design. Additionally, the use of <strong>convex optimization</strong> ensured that drones maintained stable positions without collisions.
      </p>
    
      <!-- Visual Appeal -->
      <p class="text-lg text-justify leading-relaxed px-2 mt-4">
        This emotion-driven formation mechanism not only achieves practical interaction, but also demonstrates expressive robotic behavior. EchoSwarm opens up new design opportunities in fields such as <strong>emotion-based robotics, therapy, public displays, interactive AI art</strong>, and <strong>real-time alert signaling</strong>.
      </p>
    
    </section>

    <section id="demo" class="section">
      <h2 class="text-3xl font-bold">Demo of Simulation</h2>
      <p class="mt-4 text-lg">Check out the video demo of our CoppeliaSim environment:</p>
      <div class="mt-4">
        <iframe width="560" height="315" src="images/demo.mp4" title="Demo" class="rounded-xl shadow-md" allowfullscreen></iframe>
      </div>
    </section>

    <section id="conclusion" class="section bg-white rounded-xl shadow-md p-6">
      <h2 class="text-3xl font-bold text-gray-800 mb-4">Conclusion and Future Work</h2>
      
      <!-- Conclusion -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-4 mb-2">1. Conclusion</h3>
      <p class="text-lg text-justify leading-relaxed px-2">
        In this project, we developed <strong>EchoSwarm</strong>, a novel approach to multi-robot coordination through EEG signals. By utilizing convex optimization techniques, AI-driven algorithms, and MFC, we enabled a group of drones to autonomously form shapes or letters based on the user's mental commands. The integration of CoppeliaSim allowed us to simulate the drone movements with real-time coordination, achieving an intuitive and precise control mechanism. We also implemented key features such as collision avoidance, ensuring that the drones moved cohesively while avoiding interference with one another. Our results demonstrate that EEG signals can effectively serve as a reliable input source for controlling a swarm of robots, showcasing the potential for more intuitive human-robot interaction in future applications.
      </p>
      
      <p class="text-lg text-justify leading-relaxed px-2 mt-4">
        The project achieved its main goals of forming target shapes with multiple drones using real-time EEG signals, proving the viability of using brain-computer interfaces for advanced robotics coordination. The combination of convex optimization, AI, and simulation tools such as CoppeliaSim has shown promise in pushing the boundaries of what is achievable in autonomous drone swarm systems.
      </p>
      
      <img src="images/final.PNG" alt="EchoSwarm Final Results" class="rounded-lg shadow-md my-4 w-full md:w-2/3 mx-auto" />
      
      <!-- Future Work -->
      <h3 class="text-2xl font-semibold text-gray-700 mt-6 mb-2">2. Future Work</h3>
      <p class="text-lg text-justify leading-relaxed px-2">
        While the current implementation provides a solid foundation, there are several areas where <strong>EchoSwarm</strong> can be enhanced for broader applications:
      </p>
      
      <ul class="list-disc list-inside text-lg ml-8 my-4">
        <li class="mb-3">
          <strong>Improved Signal Processing:</strong> The accuracy of EEG signals could be further refined by incorporating more advanced signal processing techniques, such as deep learning-based classification models, to reduce noise and increase the reliability of the signals in real-time scenarios.
        </li>
        
        <li class="mb-3">
          <strong>Expansion of Swarm Capabilities:</strong> Scaling up the number of drones in the swarm could be explored, introducing more complex coordination patterns, dynamic shape generation, and decentralized decision-making processes to enhance swarm autonomy.
        </li>
        
        <li class="mb-3">
          <strong>Real-World Testing:</strong> Conducting real-world tests with actual drones and EEG headsets would provide a more accurate assessment of the system's feasibility in a practical environment. This would also involve considering environmental factors such as GPS accuracy, wind, and battery life.
        </li>
        
        <li class="mb-3">
          <strong>Integration with Other Sensors:</strong> Incorporating additional sensors such as LIDAR or cameras for environmental perception could enable drones to not only follow human commands but also autonomously adapt to changing environments and obstacles, thus enhancing swarm intelligence.
        </li>
        
        <li class="mb-3">
          <strong>Enhanced User Interaction:</strong> Future versions of <strong>EchoSwarm</strong> could incorporate a more intuitive user interface that allows for additional types of input, such as gestures or speech, in combination with EEG signals, enabling more versatile and adaptive control mechanisms.
        </li>
        
        <li class="mb-3">
          <strong>Real-Time Learning:</strong> Implementing reinforcement learning could allow drones to adapt and optimize their coordination strategies based on past experiences, improving their efficiency and robustness over time.
        </li>
      </ul>
      
      <p class="text-lg text-justify leading-relaxed px-2 mt-4">
        By addressing these areas, <strong>EchoSwarm</strong> has the potential to evolve into a more sophisticated and versatile system, with applications ranging from search-and-rescue missions to collaborative industrial tasks, making it a step closer to realizing the full potential of human-robot collaboration.
      </p>
      
    </section>

    <section id="references" class="section">
      <h2 class="text-3xl font-bold">References</h2>
      <ol style="font-family: Arial, sans-serif; line-height: 1.6; margin-left: 20px;">
        <li style="margin-bottom: 10px;">1. J. Smith, "Brain-Computer Interfaces for Human-Robot Interaction," <em>Journal of Robotics and Automation</em>, vol. 10, no. 2, pp. 115-130, 2022.</li>
        <li style="margin-bottom: 10px;">2. K. Lee and A. Brown, "Convex Optimization in Multi-Robot Coordination," <em>International Journal of Robotics Research</em>, vol. 20, no. 3, pp. 245-259, 2021.</li>
        <li style="margin-bottom: 10px;">3. P. Zhang, "Signal Processing for EEG Applications in Robotics," <em>IEEE Transactions on Neural Systems and Rehabilitation Engineering</em>, vol. 29, no. 6, pp. 1142-1154, 2023.</li>
        <li style="margin-bottom: 10px;">4. S. Wang et al., "A Survey on Autonomous Drone Swarms," <em>IEEE Access</em>, vol. 8, pp. 88745-88759, 2020.</li>
        <li style="margin-bottom: 10px;">5. R. Clark, "CoppeliaSim: An Open-Source Robot Simulation Tool," <em>IEEE Robotics & Automation Magazine</em>, vol. 29, no. 4, pp. 72-83, 2019.</li>
        <li style="margin-bottom: 10px;">6. M. Johnson, "Swarm Robotics: Principles and Applications," <em>Springer Series in Robotics</em>, vol. 45, pp. 1-20, 2021.</li>
        <li style="margin-bottom: 10px;">7. H. Liu, "Optimization Methods for Swarm Robotics Systems," <em>Journal of Multi-Agent and Grid Systems</em>, vol. 15, no. 1, pp. 85-98, 2022.</li>
        <li style="margin-bottom: 10px;">8. Y. Chen and X. Yang, "Real-Time Coordination of Multi-Robot Systems via Brain Signals," <em>International Conference on Robotics and Automation (ICRA)</em>, pp. 1347-1352, 2023.</li>
        <li style="margin-bottom: 10px;">9. R. Patel, "Collision Avoidance in Autonomous Drones," <em>International Journal of Aerospace Engineering</em>, vol. 37, no. 9, pp. 1300-1312, 2022.</li>
        <li style="margin-bottom: 10px;">10. A. Harris and N. Singh, "Machine Learning for Real-Time Brain Signal Processing," <em>IEEE Transactions on Biomedical Engineering</em>, vol. 67, no. 8, pp. 2200-2212, 2023.</li>
        <li style="margin-bottom: 10px;">11. L. Davies and K. Kumar, "Deep Learning for Robot Path Planning," <em>Journal of Artificial Intelligence</em>, vol. 18, no. 4, pp. 145-160, 2021.</li>
        <li style="margin-bottom: 10px;">12. C. Foster, "Swarm Intelligence and Optimization," <em>Springer Handbook of Robotics</em>, pp. 303-324, 2020.</li>
        <li style="margin-bottom: 10px;">13. P. Mitchell et al., "Multi-Robot Coordination in Dynamic Environments," <em>Autonomous Robots</em>, vol. 22, no. 6, pp. 75-90, 2021.</li>
        <li style="margin-bottom: 10px;">14. F. Evans, "Real-Time Control of UAVs Using Brainwave Interfaces," <em>International Journal of Robotics</em>, vol. 25, no. 2, pp. 221-230, 2022.</li>
        <li style="margin-bottom: 10px;">15. S. R. McDonald, "Coordination Algorithms for Autonomous Vehicles," <em>IEEE Transactions on Robotics</em>, vol. 31, no. 7, pp. 654-670, 2020.</li>
        <li style="margin-bottom: 10px;">16. A. Fernandez and B. Taylor, "Simulating Multi-Robot Swarm Dynamics," <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, pp. 1191-1197, 2021.</li>
        <li style="margin-bottom: 10px;">17. C. Andrews and J. Young, "Autonomous Navigation for Drone Swarms," <em>Autonomous Systems</em>, vol. 16, no. 5, pp. 402-415, 2020.</li>
        <li style="margin-bottom: 10px;">18. S. Thomas, "Optimization Techniques for Robot Formation Control," <em>IEEE Robotics and Automation Letters</em>, vol. 9, no. 3, pp. 530-536, 2023.</li>
        <li style="margin-bottom: 10px;">19. R. Wang, "A Review on Brain-Computer Interface for Robotics," <em>Journal of Control and Robotics</em>, vol. 17, pp. 120-133, 2022.</li>
        <li style="margin-bottom: 10px;">20. M. Chen et al., "EEG-Based Control of Multi-Robot Systems," <em>Journal of Robotics and AI</em>, vol. 21, pp. 185-196, 2021.</li>
      </ol>
      
      
    </section>
  </main>

  <footer class="bg-gray-800 text-white text-center p-6 mt-12">
    <p class="text-lg">Contributors: Akhilesh M, Naren B S, Nitin Krishna, Gaurav Mahesh</p>
    <p class="text-sm mt-1">Multi-Functional Control in Robotics Project, 2025</p>
  </footer>

</body>
</html>
